{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic recommender\n",
    "\n",
    "We will be implementing a very simple recommender system on the movie lens dataset\n",
    "\n",
    "# 1. Recommendations with User Ratings\n",
    "\n",
    "In this first part, we're going to build a non-personalized recommender based on user ratings.  In many online platforms, such as Amazon, IMDb, and MovieLens, users are able to express their preference to items by explicit ratings (like by assigning a 1-5 star rating to a movie). We're going to use those ratings to generate a recommendation.\n",
    "\n",
    "For this part, we will:\n",
    "\n",
    "* load and process the MovieLens 1M dataset, \n",
    "* build the non-personalized recommender, and \n",
    "* evaluate the recommender."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1a: Load and process the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "   UserID  MovieID  Rating  Timestamp\n",
       "0       1     1193       5  978300760\n",
       "1       1      661       3  978302109\n",
       "2       1      914       3  978301968\n",
       "3       1     3408       4  978300275\n",
       "4       1     2355       5  978824291"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>UserID</th>\n      <th>MovieID</th>\n      <th>Rating</th>\n      <th>Timestamp</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>1193</td>\n      <td>5</td>\n      <td>978300760</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>661</td>\n      <td>3</td>\n      <td>978302109</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1</td>\n      <td>914</td>\n      <td>3</td>\n      <td>978301968</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1</td>\n      <td>3408</td>\n      <td>4</td>\n      <td>978300275</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1</td>\n      <td>2355</td>\n      <td>5</td>\n      <td>978824291</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 1
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data_df = pd.read_csv('./ratings.dat', sep='::', names=[\"UserID\", \"MovieID\", \"Rating\", \"Timestamp\"])\n",
    "data_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get some more info about the dimensions of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "unique users:  6040\nunique movies:  3706\ntotal ratings:  1000209\n"
     ]
    }
   ],
   "source": [
    "# count and print how many unique users, unique movies, and ratings in this dataset\n",
    "# Your Code Here...\n",
    "print(\"unique users: \", data_df['UserID'].unique().size)\n",
    "print(\"unique movies: \", data_df['MovieID'].unique().size)\n",
    "print(\"total ratings: \", data_df['Rating'].size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because in Python, the index for a list starts from 0, it is more convenient if we have the ids of users and movies start from 0 as well. Moreover, we also need to make sure the UserID and MovieID are continuous, so in the next cell, we reindex UserID and MovieID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, generate dictionaries for mapping old id to new id for users and movies\n",
    "unique_MovieID = data_df['MovieID'].unique()\n",
    "unique_UserID = data_df['UserID'].unique()\n",
    "j = 0\n",
    "user_old2new_id_dict = dict()\n",
    "for u in unique_UserID:\n",
    "    user_old2new_id_dict[u] = j\n",
    "    j += 1\n",
    "j = 0\n",
    "movie_old2new_id_dict = dict() \n",
    "for i in unique_MovieID:\n",
    "    movie_old2new_id_dict[i] = j\n",
    "    j += 1\n",
    "    \n",
    "# Then, use the generated dictionaries to reindex UserID and MovieID in the data_df\n",
    "for j in range(len(data_df)):\n",
    "    data_df.at[j, 'UserID'] = user_old2new_id_dict[data_df.at[j, 'UserID']]\n",
    "    data_df.at[j, 'MovieID'] = movie_old2new_id_dict[data_df.at[j, 'MovieID']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can randomly split into train/test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "2800584\n1200252\nno overlap!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# generate train_df with 70% samples and test_df with 30% samples, and there should have no overlap between them.\n",
    "# Your Code Here...\n",
    "\n",
    "train_df = data_df.sample(frac=0.7, random_state=0)\n",
    "test_df = data_df.drop(train_df.index)\n",
    "\n",
    "print(train_df.size)\n",
    "print(test_df.size)\n",
    "\n",
    "if(train_df.size + test_df.size == data_df.size):\n",
    "    print(\"no overlap!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to work with numpy array variables, and missing entries are filled with zeros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import coo_matrix\n",
    "\n",
    "num_user = len(data_df['UserID'].unique())\n",
    "num_movie = len(data_df['MovieID'].unique())\n",
    "\n",
    "train_mat = coo_matrix((train_df['Rating'].values, (train_df['UserID'].values, train_df['MovieID'].values)), shape=(num_user, num_movie)).toarray().astype(float)\n",
    "test_mat = coo_matrix((test_df['Rating'].values, (test_df['UserID'].values, test_df['MovieID'].values)), shape=(num_user, num_movie)).toarray().astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1b: Build the non-personalized recommender\n",
    "\n",
    "This model is very simple: for each movie, we calculate the average rating of this movie in the training dataset, and use this average rating as the prediction for all users with respect to this movie. In this way, the prediction will be the same across all users, i.e., it is non-personalized.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(6040, 3706)"
      ]
     },
     "metadata": {},
     "execution_count": 241
    }
   ],
   "source": [
    "\n",
    "# compute average of train_mat ignoring zeros\n",
    "train_mat[train_mat==0] = np.nan\n",
    "test_mat[test_mat==0] = np.nan\n",
    "\n",
    "total_rating_avg = np.nanmean(train_mat)\n",
    "\n",
    "# fill in any columns that are missing\n",
    "for movie_id in range(num_movie):\n",
    "    movie_column = train_mat[:,movie_id]\n",
    "    if np.count_nonzero(np.isnan(movie_column)) == num_user:\n",
    "        train_mat[:,movie_id] = total_rating_avg\n",
    "\n",
    "# initialize ratings array to be filled\n",
    "prediction_row = np.zeros((num_movie),dtype=float)\n",
    "\n",
    "# average for each row ignoring zeros\n",
    "for column in range(num_movie):\n",
    "        rating_mean = np.nanmean(train_mat[:,column])\n",
    "        prediction_row[column] = rating_mean\n",
    "\n",
    "# clone single row for every user now\n",
    "prediction_mat = np.array([prediction_row for i in range(num_user)])\n",
    "\n",
    "prediction_mat.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Top 5 movie Ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "3610 5.0\n3701 5.0\n3418 5.0\n3693 5.0\n2562 5.0\n"
     ]
    }
   ],
   "source": [
    "# this will sort prediction_mat in descending order and grab the 1st 5 indices\n",
    "# I am working with prediction_row instead of mat for simplicity\n",
    "top_5_indices = (-prediction_row).argsort()[:5]\n",
    "\n",
    "for movie_id in top_5_indices:\n",
    "    print(movie_id, prediction_row[movie_id])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##1c: Evaluate the non-personalized recommender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.9807496351980143"
      ]
     },
     "metadata": {},
     "execution_count": 242
    }
   ],
   "source": [
    "# get rmse for predictions\n",
    "\n",
    "def get_rmse(prediction_mat, test_mat):\n",
    "    # sqrt of average of squared differences\n",
    "\n",
    "    sum_squared_diff = 0\n",
    "    n = 0\n",
    "\n",
    "    for user_id in range(num_user):\n",
    "        for movie_id in range(num_movie):\n",
    "\n",
    "            actual = test_mat[user_id][movie_id]\n",
    "            predict = prediction_mat[user_id][movie_id]\n",
    "\n",
    "            if not np.isnan(actual):\n",
    "\n",
    "                sum_squared_diff += (actual - predict)**2\n",
    "                n += 1\n",
    "\n",
    "    return np.sqrt(sum_squared_diff/n)\n",
    "\n",
    "rmse = get_rmse(prediction_mat,test_mat)\n",
    "rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Recommendations with implicit feedback (50 points total)\n",
    "\n",
    "In many scenarios, we may not have explcit ratings. But we often have lots of implicit feedback. For this part, we're going to build a simple non-personalized implicit recommendation algorithm. Since feedback like user clicks, purchases, and views is much more widespread than explicit ratings, implicit recommenders offer great opportunities for far-reaching impact. \n",
    "\n",
    "Concretely, the task of implicit recommendation is to recommend items to users based on implicit signals from users, i.e., we only know what items a user is interested in, but have no idea what items the user dislikes. So for this case, the dataset we could use for this implicit recommendation experiment only contains binary data with 1 representing that the user likes the item, and with 0 representing that we don't know the user's preference towards the item. Because of this, we cannot use the same evaluation method as explicit recommendation. Instead, we need to evaluate the implicit recommendation quality by a ranking task.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2a: Process the data\n",
    "\n",
    "If there is a rating, cast as a 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(6040, 3706)"
      ]
     },
     "metadata": {},
     "execution_count": 243
    }
   ],
   "source": [
    "# Your Code Here...\n",
    "#train_df.drop(['Rating'],axis=1)\n",
    "\n",
    "train_mat[np.isnan(train_mat)==False] = 1\n",
    "test_mat[np.isnan(test_mat)==False] = 1\n",
    "\n",
    "train_mat.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2b: Build the non-personalized recommender\n",
    "\n",
    "In this part, you need to build a non-personalized recommendation model to provide a ranked list of 50 movies as the recommendation for each user. The model is very simple: for each user, the recommendation list is to rank the unwatched movies by their **popularity**, where the popularity is the number of implicit feedback each movie gets. In this case, although it is non-personalized recommender, the recommendation results may be different for users because the unwatched movies are different across users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-193023315855>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Generate a ranked list of movies by the popularity based recommendation algorithm.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mtrain_user_ranking\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_user\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# Sum up movies, ignoring nans\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "# Generate a ranked list of movies by the popularity based recommendation algorithm. \n",
    "\n",
    "train_user_ranking = np.empty((num_user,50),dtype=(int,2))\n",
    "\n",
    "# Sum up movies, ignoring nans\n",
    "movie_ranking = np.nansum(train_mat, axis=0)\n",
    "movie_ranking.astype(int)\n",
    "\n",
    "# I was a little irritated numpy doesn't have a parameter for descending sort!\n",
    "# Anyways, we sort and reverse array for descending order\n",
    "# efficient implementation of algo using insort?\n",
    "\n",
    "for user_index in range(num_user):\n",
    "\n",
    "    # np array of only unwatched, nan values. [0] at end bc it made a tuple\n",
    "    user_unwatched = np.where(np.isnan(train_mat[user_index]))[0]\n",
    "\n",
    "    # make np array of (movie id, movie ranking) for all of the unwatched movies. sort descending\n",
    "    user_unwatched_ranked = [(movie_id, movie_ranking[movie_id]) for movie_id in user_unwatched]\n",
    "    user_unwatched_ranked.sort(key = lambda x: x[1], reverse=True)\n",
    "\n",
    "    # cast as np array of tuples, then assign user index to array\n",
    "    train_user_ranking[user_index] = np.asarray(user_unwatched_ranked[:50],dtype=(int,2))\n",
    "\n",
    "\n",
    "# print out the id and popularity of the top5 movies for the first user.\n",
    "\n",
    "for i in range(5):\n",
    "    print(train_user_ranking[0][i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2c: Evaluate the non-personalized recommender \n",
    "\n",
    "In this part, we evaluate the non-personalized recommendation by the held-out testing dataset test_mat for each user. For the implicit recommendation, two typical metrics are recall@k and precision@k. Here, we will calculate recall@k and  precision@k for k=5, 20, 50 for each user, i.e., six metrics for every user. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "recall_5        0.037529\n",
       "precision_5     0.273808\n",
       "recall_20       0.108869\n",
       "precision_20    0.211283\n",
       "recall_50       0.194742\n",
       "precision_50    0.157930\n",
       "dtype: float64"
      ]
     },
     "metadata": {},
     "execution_count": 256
    }
   ],
   "source": [
    "# Calculate recall@k and precision@k with k=5, 20, 50 and print out the average over all users for these 9 metrics.\n",
    "\n",
    "def recall_k(user_id, k):\n",
    "\n",
    "    num_relevant = 0\n",
    "\n",
    "    for i in range(k):\n",
    "\n",
    "        # get top k movie ids for each user\n",
    "        predicted = train_user_ranking[user_id][i]\n",
    "        movie_id = predicted[0]\n",
    "\n",
    "        # count all relevant values that are in top k\n",
    "        if test_mat[user_id][movie_id] == 1:\n",
    "            num_relevant += 1\n",
    "    \n",
    "    # count all relevant values\n",
    "    total_relevant = np.count_nonzero(test_mat[user_id]==1)\n",
    "\n",
    "    return num_relevant/total_relevant\n",
    "\n",
    "        \n",
    "def precision_k(user_id, k):\n",
    "\n",
    "    num_relevant = 0\n",
    "\n",
    "    for rank in range(k):\n",
    "\n",
    "        # get top k movie ids for each user\n",
    "        predicted = train_user_ranking[user_id][rank]\n",
    "        movie_id = predicted[0]\n",
    "\n",
    "        # count all relevant values in top k\n",
    "        if test_mat[user_id][movie_id] ==1:\n",
    "            num_relevant += 1\n",
    "    \n",
    "    return num_relevant/k\n",
    "\n",
    "\n",
    "validation_df = {}\n",
    "\n",
    "for k in [5,20,50]:\n",
    "\n",
    "        # make empty lists to contain precision and recal scores\n",
    "        recall_list = []\n",
    "        precision_list = []\n",
    "\n",
    "        for user in range(num_user):\n",
    "\n",
    "            # check that a user in test_mat does not have all nan values\n",
    "            if np.all(np.isnan(test_mat[user])) == False:\n",
    "\n",
    "                # add recall and precision scores to list\n",
    "                recall_list.append(recall_k(user,k))\n",
    "                precision_list.append(precision_k(user,k))\n",
    "\n",
    "        # add lists to dictionary\n",
    "        validation_df[f\"recall_{k}\"] = recall_list\n",
    "        validation_df[f\"precision_{k}\"] = precision_list\n",
    "\n",
    "validation_df = pd.DataFrame(validation_df)\n",
    "validation_df.mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "16\n"
     ]
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
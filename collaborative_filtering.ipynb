{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collaborative Filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Recommendations with User Ratings \n",
    "\n",
    "In this first part,  we still focus on the rating prediction recommendation task with explicit feedback. We will need to build **personalized** models as opposed to non personalized ones from before.\n",
    "\n",
    "For this part, we will:\n",
    "\n",
    "* load and process the MovieLens 1M dataset, \n",
    "* build a baseline estimation model,\n",
    "* build a user-user collaborative filtering model,\n",
    "* improve the user-user collaborative filtering model, and\n",
    "* evaluate and compare these different models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocess as before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "<ipython-input-1-e069bfd8a143>:5: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n  data_df = pd.read_csv('./ratings.dat', sep='::', names=[\"UserID\", \"MovieID\", \"Rating\", \"Timestamp\"])\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.sparse import coo_matrix\n",
    "\n",
    "data_df = pd.read_csv('./ratings.dat', sep='::', names=[\"UserID\", \"MovieID\", \"Rating\", \"Timestamp\"])\n",
    "\n",
    "# First, generate dictionaries for mapping old id to new id for users and movies\n",
    "unique_MovieID = data_df['MovieID'].unique()\n",
    "unique_UserID = data_df['UserID'].unique()\n",
    "j = 0\n",
    "user_old2new_id_dict = dict()\n",
    "for u in unique_UserID:\n",
    "    user_old2new_id_dict[u] = j\n",
    "    j += 1\n",
    "j = 0\n",
    "movie_old2new_id_dict = dict()\n",
    "for i in unique_MovieID:\n",
    "    movie_old2new_id_dict[i] = j\n",
    "    j += 1\n",
    "    \n",
    "# Then, use the generated dictionaries to reindex UserID and MovieID in the data_df\n",
    "user_list = data_df['UserID'].values\n",
    "movie_list = data_df['MovieID'].values\n",
    "for j in range(len(data_df)):\n",
    "    user_list[j] = user_old2new_id_dict[user_list[j]]\n",
    "    movie_list[j] = movie_old2new_id_dict[movie_list[j]]\n",
    "data_df['UserID'] = user_list\n",
    "data_df['movieID'] = movie_list\n",
    "\n",
    "# generate train_df with 70% samples and test_df with 30% samples, and there should have no overlap between them.\n",
    "train_index = np.random.random(len(data_df)) <= 0.7\n",
    "train_df = data_df[train_index]\n",
    "test_df = data_df[~train_index]\n",
    "\n",
    "# generate train_mat and test_mat\n",
    "num_user = len(data_df['UserID'].unique())\n",
    "num_movie = len(data_df['MovieID'].unique())\n",
    "\n",
    "train_mat = coo_matrix((train_df['Rating'].values, (train_df['UserID'].values, train_df['MovieID'].values)), shape=(num_user, num_movie)).astype(float).toarray()\n",
    "test_mat = coo_matrix((test_df['Rating'].values, (test_df['UserID'].values, test_df['MovieID'].values)), shape=(num_user, num_movie)).astype(float).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1a: Build the Baseline Estimation Model\n",
    "\n",
    "First, let's implement a simple personalized recommendation model -- the baseline estimate : $b_{u,i}=\\mu+b_i+b_u$, where $\\mu$ is the overall mean rating for all items, $b_u$ = average rating of user $u-\\mu$, $b_i$ = average rating of item $i-\\mu$. We store the prediction as a numpy array variable 'prediction_mat' of size (#users, #movies) with each entry showing the predicted rating for the corresponding user-movie pair."
   ]
  },
  {
   "source": [],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "<ipython-input-2-87099c776fc4>:11: RuntimeWarning: Mean of empty slice\n  b_i = np.nanmean(train_mat, axis = 0).reshape(1,num_movie) - u[0].reshape(1,num_movie)\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(array([[4.95751623, 4.04074232, 4.70622401, ..., 1.56084956, 5.56084956,\n",
       "                nan],\n",
       "        [4.53044856, 3.61367465, 4.27915634, ..., 1.13378189, 5.13378189,\n",
       "                nan],\n",
       "        [4.57223484, 3.65546094, 4.32094262, ..., 1.17556818, 5.17556818,\n",
       "                nan],\n",
       "        ...,\n",
       "        [4.81465908, 3.89788518, 4.56336687, ..., 1.41799242, 5.41799242,\n",
       "                nan],\n",
       "        [4.68645396, 3.76968005, 4.43516174, ..., 1.28978729, 5.28978729,\n",
       "                nan],\n",
       "        [4.34684793, 3.43007402, 4.09555571, ..., 0.95018126, 4.95018126,\n",
       "                nan]]),\n",
       " (6040, 3706))"
      ]
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "source": [
    "# calculate the prediction_mat by the baseline estimation recommendation algorithm\n",
    "\n",
    "#  Convert to nans for convenience\n",
    "train_mat[train_mat==0] = np.nan\n",
    "\n",
    "total_rating_avg = np.nanmean(train_mat)\n",
    "#prediction_mat = np.empty([num_user, num_movie])\n",
    "\n",
    "u = np.empty([num_user, num_movie])\n",
    "u.fill(total_rating_avg)\n",
    "b_i = np.nanmean(train_mat, axis = 0).reshape(1,num_movie) - u[0].reshape(1,num_movie)\n",
    "b_u = np.nanmean(train_mat, axis = 1).reshape(num_user, 1) - u[:,0].reshape(num_user, 1)\n",
    "\n",
    "prediction_mat = u + b_u + b_i\n",
    "\n",
    "prediction_mat, prediction_mat.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, with this prediction_mat based on the baseline estimate, let's use RMSE to evaluate the quality of the baseline estimate model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.938653322433489"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "# calculate and print out the RMSE for your prediction_df and the test_df\n",
    "\n",
    "prediction_mat[np.isnan(prediction_mat)] = 0\n",
    "\n",
    "indicator_mat = (test_mat > 0).astype(float)\n",
    "test_rmse = (np.sum(((prediction_mat - test_mat) * indicator_mat) ** 2) / np.sum(indicator_mat)) ** 0.5\n",
    "\n",
    "test_rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1b: User-user Collaborative Filtering with Jaccard Similarity \n",
    "\n",
    "In this part, we need to build a user-user collaborative filtering recommendation model with **Jaccard similarity** to predict user-movie ratings. \n",
    "\n",
    "The prediction of the score for a user-item pair $(u,i)$ should use the formulation: $p_{u,i}=\\bar{r}_u+\\frac{\\sum_{u^\\prime\\in N}s(u,u^\\prime)(r_{u^\\prime,i}-\\bar{r}_{u^\\prime})}{\\sum_{u^\\prime\\in N}|s(u, u^\\prime)|}$, where $s(u, u^\\prime)$ is the Jaccard similarity. We set the size of $N$ as 10.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(array([[0.34615385, 0.        , 0.        , ..., 0.        , 0.00892857,\n",
       "         0.00374532],\n",
       "        [0.        , 0.21794872, 0.        , ..., 0.00952381, 0.00581395,\n",
       "         0.00923077],\n",
       "        [0.        , 0.        , 0.375     , ..., 0.        , 0.        ,\n",
       "         0.00377358],\n",
       "        ...,\n",
       "        [0.        , 0.00952381, 0.        , ..., 0.69230769, 0.01136364,\n",
       "         0.00826446],\n",
       "        [0.00892857, 0.00581395, 0.        , ..., 0.01136364, 0.40540541,\n",
       "         0.01967213],\n",
       "        [0.00374532, 0.00923077, 0.00377358, ..., 0.00826446, 0.01967213,\n",
       "         0.30167598]]),\n",
       " (6040, 6040))"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "# calculate the prediction_mat by your user-user collaborative filtering recommendation algorithm\n",
    "\n",
    "train_mat[train_mat==0] = np.nan\n",
    "\n",
    "bool_mat = (~np.isnan(train_mat)).astype(float)\n",
    "    \n",
    "num_rating_per_user = np.sum(bool_mat, axis=1, keepdims=True)\n",
    "numerator = np.matmul(indicator_mat, indicator_mat.T)\n",
    "denominator = num_rating_per_user + num_rating_per_user.T - numerator\n",
    "denominator[denominator==0] = 1\n",
    "jaccard_table = numerator/denominator\n",
    "\n",
    "jaccard_table, jaccard_table.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(6040, 3706)"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "# will contain avg rating for each user\n",
    "\n",
    "train_mat[train_mat==0] = np.nan\n",
    "user_means = np.nanmean(train_mat, axis = 1).T\n",
    "\n",
    "prediction_mat = np.empty([num_user, num_movie])\n",
    "\n",
    "for user_id in range(num_user):\n",
    "\n",
    "    neighborhood = (-jaccard_table[user_id]).argsort()[:10]\n",
    "    p_u = np.zeros([1, num_movie])\n",
    "    denominator = 0\n",
    "\n",
    "    for other_user in neighborhood:\n",
    "\n",
    "        r_u_prime = train_mat[other_user]\n",
    "        r_u_prime[np.isnan(r_u_prime)] = user_means[other_user]\n",
    "\n",
    "        jaccard_sim = jaccard_table[user_id][other_user]\n",
    "        diff_mean = r_u_prime - np.full((1,num_movie),user_means[other_user])\n",
    "        \n",
    "        p_u += jaccard_sim * diff_mean\n",
    "        denominator += abs(jaccard_sim)\n",
    "\n",
    "\n",
    "    prediction_mat[user_id] = p_u/denominator + user_means[user_id]\n",
    "\n",
    "prediction_mat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "1.0122187582515219"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "# calculate and print out the RMSE for prediction_df and the test_df\n",
    "\n",
    "prediction_mat[np.isnan(prediction_mat)] = 0\n",
    "\n",
    "indicator_mat = (test_mat > 0).astype(float)\n",
    "test_rmse = (np.sum(((prediction_mat - test_mat) * indicator_mat) ** 2) / np.sum(indicator_mat)) ** 0.5\n",
    "\n",
    "test_rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1c: Improve the Collaborative Filtering Model\n",
    "\n",
    "Now we try to see if we can improve Collaborative Filtering with different similarity metrics + item-item comparisons\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create item item cosine table\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "cosine_movie = np.zeros(shape=(num_movie, num_movie))\n",
    "\n",
    "for movie_id in range(num_movie):\n",
    "    for other_movie in range(num_movie):\n",
    "        cosine_movie[movie_id][other_movie] = (np.dot(train_mat[movie_id], train_mat[other_movie])/(np.linalg.norm(train_mat[movie_id])*np.linalg.norm(train_mat[other_movie])))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(6040, 3706)"
      ]
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "train_mat[train_mat==0] = np.nan\n",
    "train_mat = train_mat.T\n",
    "movie_means = np.nanmean(train_mat, axis = 0)\n",
    "prediction_mat = np.empty([num_movie, num_user])\n",
    "\n",
    "for movie_id in range(num_movie):\n",
    "\n",
    "    neighborhood = (-cosine_movie[movie_id]).argsort()[:10]\n",
    "    numerator = np.zeros([1, num_user])\n",
    "    denominator = 0\n",
    "\n",
    "    for other_movie in neighborhood:\n",
    "\n",
    "        r_m_prime = train_mat[other_movie]\n",
    "        r_m_prime[np.isnan(r_m_prime)] = movie_means[other_movie]\n",
    "\n",
    "        cosine_sim = cosine_movie[movie_id][other_movie]\n",
    "        numerator += cosine_sim * r_m_prime\n",
    "\n",
    "        denominator += abs(cosine_sim)\n",
    "\n",
    "\n",
    "    prediction_mat[movie_id] = numerator/denominator \n",
    "\n",
    "\n",
    "prediction_mat = prediction_mat.T\n",
    "train_mat = train_mat.T\n",
    "prediction_mat.shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "1.0390849247045915"
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "prediction_mat[np.isnan(prediction_mat)] = 0\n",
    "\n",
    "indicator_mat = (test_mat > 0).astype(float)\n",
    "test_rmse = (np.sum(((prediction_mat - test_mat) * indicator_mat) ** 2) / np.sum(indicator_mat)) ** 0.5\n",
    "\n",
    "test_rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Recommendations with implicit feedback\n",
    "\n",
    " Now we implement a user-user collaborative filtering algorithm for recommendation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explicit -> Implicit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mat = (train_mat > 0).astype(float)\n",
    "test_mat = (test_mat > 0).astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The predicted preference score from user $u$ to movie $i$ can be calculated as: $p_{u,i}=\\frac{\\sum_{u^\\prime\\in N}s(u,u^\\prime)r_{u^\\prime,i}}{\\sum_{u^\\prime\\in N}|s(u,u^\\prime)|}$, where $s(u,u^\\prime)$ is the cosine similarity, and we set the size of $N$ as 10.\n",
    "\n",
    "Using this knowledge, generate our ranked lists for top 50 movies/user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create user user cosine table\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "cosine_user = np.zeros(shape=(num_user, num_user))\n",
    "\n",
    "for user_id in range(num_user):\n",
    "    for other_user in range(num_user):\n",
    "        cosine_user[user_id][other_user] = (np.dot(train_mat[user_id], train_mat[other_user])/(np.linalg.norm(train_mat[user_id])*np.linalg.norm(train_mat[other_user])))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[1., 1., 1., ..., 1., 1., 1.],\n",
       "       [1., 1., 1., ..., 1., 1., 1.],\n",
       "       [1., 1., 1., ..., 1., 1., 1.],\n",
       "       ...,\n",
       "       [1., 1., 1., ..., 1., 1., 1.],\n",
       "       [1., 1., 1., ..., 1., 1., 1.],\n",
       "       [1., 1., 1., ..., 1., 1., 1.]])"
      ]
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "source": [
    "cosine_user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_mat = np.empty([num_user, num_movie])\n",
    "#user_means = np.nanmean(train_mat, axis = 1).T\n",
    "\n",
    "for user_id in range(num_user):\n",
    "\n",
    "    neighborhood = (-cosine_user[user_id]).argsort()[:10]\n",
    "    numerator = np.zeros([1, num_movie])\n",
    "    denominator = 0\n",
    "\n",
    "    for other_user in neighborhood:\n",
    "\n",
    "        r_u_prime = train_mat[other_user]\n",
    "        #r_u_prime[np.isnan(r_u_prime)] = user_means[other_user]\n",
    "\n",
    "        cosine_sim = cosine_user[user_id][other_user]\n",
    "        numerator += cosine_sim * r_u_prime\n",
    "\n",
    "        denominator += abs(cosine_sim)\n",
    "\n",
    "\n",
    "    prediction_mat[user_id] = numerator/denominator \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[1., 1., 1., ..., 1., 1., 1.],\n",
       "       [1., 1., 1., ..., 1., 1., 1.],\n",
       "       [1., 1., 1., ..., 1., 1., 1.],\n",
       "       ...,\n",
       "       [1., 1., 1., ..., 1., 1., 1.],\n",
       "       [1., 1., 1., ..., 1., 1., 1.],\n",
       "       [1., 1., 1., ..., 1., 1., 1.]])"
      ]
     },
     "metadata": {},
     "execution_count": 23
    }
   ],
   "source": [
    "prediction_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[1259., 1260., 1251., ..., 1220., 1219., 3705.],\n",
       "       [1259., 1260., 1251., ..., 1220., 1219., 3705.],\n",
       "       [1259., 1260., 1251., ..., 1220., 1219., 3705.],\n",
       "       ...,\n",
       "       [1259., 1260., 1251., ..., 1220., 1219., 3705.],\n",
       "       [1259., 1260., 1251., ..., 1220., 1219., 3705.],\n",
       "       [1259., 1260., 1251., ..., 1220., 1219., 3705.]])"
      ]
     },
     "metadata": {},
     "execution_count": 19
    }
   ],
   "source": [
    "prediction_mat.astype(int)\n",
    "\n",
    "top_50_ranked = np.empty([num_user, 50])\n",
    "\n",
    "for user_id in range(num_user):\n",
    "    top_50_ranked[user_id] = np.argpartition(prediction_mat[user_id], -50)[-50:]\n",
    "\n",
    "top_50_ranked"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaulate our recommender with recall and precision at k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "recall_5        0.001675\n",
       "precision_5     0.020927\n",
       "recall_20       0.006730\n",
       "precision_20    0.018932\n",
       "recall_50       0.016907\n",
       "precision_50    0.018821\n",
       "dtype: float64"
      ]
     },
     "metadata": {},
     "execution_count": 20
    }
   ],
   "source": [
    "# Calculate recall@k, precision@k with k=5, 20, 50 and print out the average over all users for these 6 metrics.\n",
    "\n",
    "def recall_k(user_id, k):\n",
    "\n",
    "    num_relevant = 0\n",
    "\n",
    "    for i in range(k):\n",
    "\n",
    "        # get top k movie ids for each user\n",
    "        movie_id = int(top_50_ranked[user_id][i])\n",
    "\n",
    "        # count all relevant values that are in top k\n",
    "        if test_mat[user_id][movie_id] == 1:\n",
    "            num_relevant += 1\n",
    "    \n",
    "    # count all relevant values\n",
    "    total_relevant = np.count_nonzero(test_mat[user_id]==1)\n",
    "\n",
    "    return num_relevant/total_relevant\n",
    "\n",
    "def precision_k(user_id, k):\n",
    "\n",
    "    num_relevant = 0\n",
    "\n",
    "    for rank in range(k):\n",
    "\n",
    "        # get top k movie ids for each user\n",
    "        movie_id =int(top_50_ranked[user_id][rank])\n",
    "\n",
    "        # count all relevant values in top k\n",
    "        if test_mat[user_id][movie_id] ==1:\n",
    "            num_relevant += 1\n",
    "    \n",
    "    return num_relevant/k\n",
    "validation_df = {}\n",
    "\n",
    "for k in [5,20,50]:\n",
    "\n",
    "        # make empty lists to contain precision and recal scores\n",
    "        recall_list = []\n",
    "        precision_list = []\n",
    "\n",
    "        for user in range(num_user):\n",
    "\n",
    "            # check that a user in test_mat does not have all nan values\n",
    "            if np.all(test_mat[user]==0) == False:\n",
    "\n",
    "                # add recall and precision scores to list\n",
    "                recall_list.append(recall_k(user,k))\n",
    "                precision_list.append(precision_k(user,k))\n",
    "\n",
    "        # add lists to dictionary\n",
    "        validation_df[f\"recall_{k}\"] = recall_list\n",
    "        validation_df[f\"precision_{k}\"] = precision_list\n",
    "\n",
    "validation_df = pd.DataFrame(validation_df)\n",
    "validation_df.mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "name": "python385jvsc74a57bd04bba1d32cad56bd564e8db7f54bce5a3582261770c0fd88a009216e68a1efe96",
   "display_name": "Python 3.8.5 64-bit ('Aditya': virtualenv)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}